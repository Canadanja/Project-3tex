\section{The method}\label{sec:algo}
As presented in the previous section the eigenvalue problem of two electrons in a harmonic oscillator without any interaction terms can be easily solved. Looking at electrons in a quantum dot this calculation gains complexity. This is why we introduce the Variational Monte Carlo Method for estimating the electron states.
\subsection{Variational Monte Carlo method}
In the Variational principle, we take the eigenvalue problem form equation~\ref{glg:eigenvalue} and expand the wave function as following:
\begin{equation}
\varphi_0 = \sum_{\lambda=0}^{\infty} c_{0 \lambda} \psi_{\lambda},
\end{equation}
where $c_{0 \lambda}$ are coefficients.\\
In quantum mechanics the energy is the expectation value
\begin{align}
E = &\frac{\langle \psi_0 | \hat{H} | \psi_0 \rangle}{\langle \psi_0 | \psi_0 \rangle}\\
\intertext{So when we apple the expansion to this, we get:}
&\frac{\langle \varphi_0 | \hat{H} | \varphi_0 \rangle}{\langle \varphi_0 | \varphi_0 \rangle}\\
= &\frac{\sum_{\alpha,\beta}c_{0\alpha}^*c_{0 \beta} \int d\tau \psi_{\alpha}^*(\tau)\hat{H}\psi_{\beta}(\tau)}{\sum_{\alpha,\beta}c_{0\alpha}^*c_{0 \beta} \int d\tau \psi_{\alpha}^*(\tau)\psi_{\beta}(\tau)}\\
= &\frac{\sum_{\alpha} E_{\alpha} |c_{0\alpha}|^2}{\sum_{\alpha} |c_{0\alpha}|^2},
\end{align}
because by construction $\langle\psi_{\alpha}| \psi_{\beta}\rangle = \delta_{\alpha \beta}$ for eigenfunctions $\psi_{\alpha},\psi_{beta}$.\\
We have to consider two cases now:
\begin{itemize}
\item If the expansion $\varphi_0$ is not the eigenfunction $\psi_0$ we get the an energy
\begin{equation}
E_0 \leqslant \frac{\langle \varphi_0 | \hat{H} | \varphi_0 \rangle}{\langle \varphi_0 | \varphi_0 \rangle}.
\end{equation}
\item If the expansion $\varphi_0$ is exact the eigenfunction $\psi_0$ we get the exact energy
\begin{equation}
E_0 = \frac{\langle \varphi_0 | \hat{H} | \varphi_0 \rangle}{\langle \varphi_0 | \varphi_0 \rangle}.
\end{equation}
\end{itemize}
In the second case the variance of the energy
\begin{equation}
\mathrm{var}(E) = \langle H^2 \rangle - \langle H\rangle^2 = 0.
\end{equation}
As the expansion wave function $\varphi_0$ we use a trial wave function we will call $\psi_T (\mathbf{r_1, r_2},\alpha, /beta)$, with $\alpha$ and $\beta$ being the variational parameters. In this report the trial wave function for two electrons has the form:
\begin{equation}\label{eq:trialwavefunction}
\psi_T(\mathbf{r_1,r_2}) = C \exp\left[-\frac{\omega}{2} (r_1^2+r_2^2)\right] \exp \left[ \frac{a_{12} r_{12}}{(1+\beta r_{12})} \right]
\end{equation}
with
\begin{align}
a_{12} =\left\{\begin{array}{cl} 1, & \mbox{for} \uparrow\downarrow\\ 1/3, & \mbox{for} \uparrow\uparrow,\downarrow\downarrow \end{array}\right.
\end{align}
And
\begin{equation}
r_{12} = \sqrt{\mathbf{r_1} - \mathbf{r_2}}
\end{equation}
 Then we will compute the expectation value $E(\alpha)$ and find its minimum or alternatively the minimum of its variance $\mathrm{var}(E(\alpha))$...
\subsection{Monte Carlo methods}
The basis of the method explained above are the Monte Carlo methods, which can be referred to as statistical simulation methods. The central building block of these methods is the propability distribution function (PDF), which is used to describe and characterize the physical problem. This does not restrict the method to statistical problems, but by displaying the desired solution in terms of PDF's, non-stochastic problems can be handled as well. During a Monte-Carlo simulation, random numbers must be generated covering an interval uniformly. Using these numbers, many random samples are taken from the PDF. In order to get the desired result the average of all samples is computed. According to this, the precision of the simulation rises with the amount of samples. The error has to be estimated to get an impression of the simulation's precision.\\
On the contrary of statistical random number-based methods in standard mathematical modelling, the problem would be distretized and solved by a numerical approach.\\
\subsubsection{Pseudo-random number generation}\label{sec:ran}
As a main ingredient, random numbers play as important role in Monte-Carlo simulations and therefore have to be 'as random as possible'. The generation of truely random numbers is not practically not possible, this is why the random numbers we work with are pseudo-random, generated by an algorithm fullfilling the criteria of
\begin{itemize}
\item generating equally distributed numbers in a given interval (usually [0,1])
\item repeating random number sequences seldom
\item being fast
\item generating insignificantly correlated numbers
\end{itemize}
I this report, we use random number generators explained in~\cite{numerical}, that are called \texttt{ran0} and \texttt{ran1}.\\
Furthermore, we generate random gaussian distributed random numbers using \texttt{gaussian} % woher kommt gaussian? Brauchen wir hier Code?
\subsection{Metropolis algorithm}\label{sec:metropolis}
The difficult part of Monte-Carlo simulations is the selection rule for random states. One must find a method when to reject and when to accept the generated state. Precision and efficiency strongly depend on this rule. Supposing we have a distribution such as the one shown in figure~\ref{fig:distribution} and we have already picked an initial random variable at $r_i$. Since we are performing the simulation on many samples, we now have to pick a new random number keeping in mind, that there are two cases, which must be avoided:
\begin{itemize}
\item Choosing repeatedly numbers very close to the initial value such as $r_j$ in figure~\ref{fig:distribution}. We would then 'get stuck' around the interval of $r_i$ and therefore loose the overview of the function we are evalutating.
\item Jumping numbers far away from the initial value, where the distribution is negligable, for example to $r_k$ in figure~\ref{fig:distribution}. 
\end{itemize}
%Grafik ist noch nicht fertig
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{distribution}
    \caption{Gaussian distribution showing problems in selection of random states}
    \label{fig:distribution}
\end{figure}
Preventing the simulation in creating biased averages and being unprecise is possible by using the Metropolis algorithm, which is a Markov process, satisfying both ergodicity and detailed balance.\\
Markov chains are referred to as random walks with selected propability to make a move, which is independent of the previous step. An example of this movement is the Brownian random walk shown in figure~\ref{fig:Brown}, where a particle moves in the $x$-$y$-plane with step length 1 preforming hundred steps. The propability of moving is the same for every direction. Using Markov processes, we can generate new random states and reach the most likely state (equilibrium) after a certain time.
%ergodicity
%detailed balance
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{Brown}
    \caption{Brownian random walk of 100 steps in $x$-$y$-plane and step length 1}
    \label{fig:Brown}
\end{figure}
\FloatBarrier
\subsubsection{Importance sampling}
There are lots of examples in science, where biasing is disturbing and should be avoided. One of them are the require unbiased uncorrelated random numbers in section~\ref{sec:ran}. In Monte-carlo methods, biasing can be tool to increase the simulation's efficiency by preforming a Metropolis walk biased by the trial wave function. Since our problem is somewhat simular to a diffusion process in one dimension for one particle, we may use an approach based on the Fokker-Planck and the Langevin equation.\\
The 'old' and 'new' positions in space can be caluclated by
\begin{align}
r_{old} &= \eta\\
r_{new} &= r_{old} + \eta + \delta t D F_{old},
\end{align}
where $\eta$ denotes a gaussian distributed random variable, $\delta t$ refers to the time step, $D$ is the diffusion constant, which is in our case set to $D=0.5$ and $F_{old}$ is the quantum force at position $r_{old}$. Note, that $\eta$ are different random numbers.\\
The term responsible for biasing the walk in space $x$ is the quantum force, which leads the walk to regions with large trial wave function. In a brute force Metropolis algorithm, the propability of moving would be the same for all directions. The quantum force is
\begin{equation}\label{eq:quantum_force}
\mathbf{F} = 2 \frac{1}{\psi_T} \mathbf{\nabla} \psi_T
\end{equation}
In order to include this biasing in the Metropolis algorithm, we will replace
\begin{align}
q(r_{old},r_{new}) &= \frac{\vert \psi_T(r_{new})\vert^2}{\vert \psi_T(r_{old})\vert ^2}
\intertext{by}
q(r_{old},r_{new}) &= \frac{G(r_{old},r_{new},\delta t) \vert \psi_T(r_{new})\vert^2}{G(r_{new},r_{old},\delta t) \vert \psi_T(r_{old})\vert^2},
\intertext{where the quantity $G$ refers to the Greensfunction}
G(y,x,\delta t) &= \frac{1}{(4\pi D\delta t)^{3N/2}} \exp\left[-(y-x-D \delta t F(x))^2 \frac{1}{4D \delta t} \right]
\end{align}


\subsection{Closed form solutions}
The quantum force (eq. \ref{eq:quantum_force}) and the kinetic energy part \todo{referenzierung auf funktion} are till now calculated with a brute force derivation. A disadvantage of this method is that the wavefunction has to be evaluated multiple times at different positions $r+h$, $r-h$ respectively. In order to optimize this step one can implement the analytical expressions. These are derived in \citet{hogberget2013}. The quantum force can thereby be expressed by
\begin{equation}
\mathbf{F_i} = 2 \left( \frac{\nabla_i |\mathbf{S\uparrow}|}{|\mathbf{S\uparrow}|} + \frac{\nabla_i J}{J} \right).
\end{equation}
The first part of the right side denotes the gradient of the Slater determinant, the second one the gradient of the Jastrow factor. It has to be payed attention that this is the expression for only one particle with index $i$ moved with spin up. Considering a spin down particle the $|\mathbf{S\uparrow}|$ becomes $|\mathbf{S\downarrow}|$. Further the kinetic part of the local energy arises as a result of
\begin{equation}
\frac{\nabla_i^2 \Psi_T}{\Psi_T} = \frac{\nabla_i^2 |\mathbf{S\uparrow}|}{|\mathbf{S\uparrow}|} + \frac{\nabla_i^2 J}{J} + 2\left( \frac{\nabla_i |\mathbf{S\uparrow}|}{|\mathbf{S\uparrow}|} \cdot \frac{\nabla_i J}{J} \right),
\end{equation}
again assuming a spin up particle.

The missing expressions can be obtained by deriving the trial wavefunction \ref{} \todo{referenz zu allgemeiner trial wavefunction}. This is also done in \citet{hogberget2013} and reveals 
\begin{equation}\label{eq:jastrow-derivations}
    \frac{\nabla_i J}{J} = \sum_{k\neq i = 1}^{N}\frac{a_{ik}}{r_{ik}} \frac{\mathbf{r_i}-\mathbf{r_k}}{\left(1 + \beta r_{ik} \right)^2}
\end{equation}
for the gradient and 
\begin{equation}
    \frac{\nabla_i^2 J}{J} = \left| \frac{\nabla_i J}{J} \right| - \sum_{k\neq i = 1}^{N}a_{ik} \frac{\beta r_{ik} - 1}{r_{ik}\left(1 +\beta r_{ik} \right)^3} 
\end{equation}
for the laplacian of the jastrow factor. The variables and constants are the already known ones of equation \ref{} \todo{referenz zu trialwavefunction f√ºr viele teilchen}. The derivations of the Slater determinants can be calculated to
\begin{equation}\label{eq:gradientslater}
    \frac{\nabla_i |\mathbf{S}|}{|\mathbf{S}|} = \sum_k \left( \nabla_i \phi_k(\mathbf{r}_i)\right)(\mathbf{S}_{ki}^{-1})
\end{equation}
for the gradient and
\begin{equation}\label{eq:laplacianslater}
    \frac{\nabla_i^2 |\mathbf{S}|}{|\mathbf{S}|} = \sum_k^{N/2} \left( \nabla_i^2 \phi_k(\mathbf{r}_i)\right)(\mathbf{S}_{ki}^{-1})
\end{equation}
for the laplacian. For the first part on the right side of eqs. \ref{eq:gradientslater} and \ref{eq:laplacianslater} exists analytical expressions which can be determined by applying the operands on the single particle equation \ref{}\todo{referenziere single particle function}. These can also be found in tabulated form in \citet[app. D]{hogberget2013}. The last term $\mathbf{S}_{ki}^{-1}$ is the transpose of the inverse Slater matrix. 

The closed form solutions are fully implemented in the code and can be found at Github on the branch \texttt{closed\_form}. Unfortunately there were still unresolved problems which yielded to non-correct energies in the solutions (details in the evaluation). 